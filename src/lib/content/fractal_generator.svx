---
title: "Fractal generator with leapmotion"
---
In my 3rd years as an engineering student, I had the the opportunity to work on a project in groups of 3.
The goal was to create a demo for usage in conventions, making use of a VR equipment.
I suggested a fractal renderer with leapmotion(hand tracking) control and that's what we settled on.
We tried two different approaches, one is raymarching of signed distance field, and the other is rendering attractors with chaos game.
Work on the raymarching was mainly carried by myself, I then contributed to optimization and shading on the attractor renderer.

## Ray Marching

### General Idea

- Raymarching is similar to raytracing in that we launch rays from the camera, but instead of testing all intersections, we simply estimate a distance we can advance without hitting anything and step the ray
- We can make use of signed distance functions(SDF), which give the euclidian distance to the nearest surface for a given position, negative values being inside the object.
- For a sphere, we can simply take the distance to the center and subtract the radius
- Some operation can be performed on one or more SDF while conserving these properties:
    - union : min of distances
    - intersection : max of distances
    - fold: mirror the position in one half of a plane, ex:absolute value to mirror around zero
    - rigid transformation
    - plane inversion
  in practice many of these transformation don't exactly preserve the properties, what matter is to have the estimated distance less or equal to the actual one and that stepping along a ray still converge to the surface
  sometimes even incorrect SDF can produce acceptable results
- Fractals mainly use the fold operation because it double the amount geometry using only one operation.
- In this project I used the Kaleidoscopic Iterated Function System (KIFS) family of fractals:
      - it fold one three axis, conveniently using abs on a vec3
      - then apply a rigid transformation
      - repeat
- here is a collection of signed distance functions to play with: https://jbaker.graphics/writings/DEC.html

### Rendering
- the shading use phong lighting plus shadow rays
- I make soft shadows by using the distance function along the ray to conpute the exposure on a point
- ambient occlusion simply use the number of raymarching steps

## Attractors

### General Idea

The initial idea was based on this video: https://www.youtube.com/watch?v=1L-x_DH3Uvg

Initially the chaos game consist of repeatedly choosing a point in a set of point and jumping half the distance to it.
Plotting all position end up converging to an attractor. In the case of 3 point, you will get a sierpinsky triangle.
It turns out for each points you have a corresponding affine transform that can represent it, so we can generalize it to arbitrary affine transforms.

For rendering, we store a large amount of points on the GPU in an SSBO.
We also have a depth buffer and a 'jump distance' texture used for shading
Then position are updated with a compute shader :
1. For each point, for each attractor compute its transformed position. If it's the nearest on the depthmap, updates the depthmap and the "jump distance" texture.
2. choose randomly using weight (see below) an attractor and updates the point position with its transform

The actual rendering is done in another compute shader. Using the informations obtained from the previous path, it generates the colored texture that is rendered.

### Weights

Idealy, we would wan't every computed point to end up as being on the surface on the fractal viewed from the camera.
Because this is obviously impossible, I tried to have a somewhat uniform density of computed point.
The first idea was to have weights proportional the the determinant of the linear transform, as it is the factor of the volume after the transformation.
A problem arise with flat transforms, where the determinant approach zero; these flat transforms still contribute to the overall shape. 
So what we need is an estimation of the area of the volume projected on screen.
I used here the sum of the faces area , which give me a reasonable and orientation independant heuristic.

The results are dramatic when rendering from example Romanesco's cabbage. It defined it by a sligntly scaled down transform and a rotated transform on the side.
This make it very unlikely for points to reach the top of the cabbage as you would need them to choose the large transform about 100 times in a row with a 50% chance.

<span style="display: inline flex;align-items: center">
<enhanced:img src="$lib/images/cabbage1.png" alt="before" style="width:calc(100% - 2em);height: 100%;"/>
<enhanced:img src="$lib/images/cabbage2.png" alt="after" style="width:calc(100% - 2em);height: 100%;"/> 
</span>

### Shading

#### general Idea

We write on a depthmap in the compute process.
We approximate normals using this depthmap by comparing neighboring depths
Then, using those normals, we can add :
- a simple phong
- handmades SSAO implementations

#### Ambient Occlusion
We created several methods for SSAO since we were initiallly too lazy to do a classic implementation by sampling random points using precomputed positions.
We later added the by the book implementation anyway.
My attempt was to sample depth in a grid around each pixels and clamp depth to compute the occlusion in a cube of fixed size.
The objective was to take into advantage the full depth value of each sample and avoid noise due to unsufficient sampling.
This gave good results but difficult to compare since fractals are complex objects that makes it difficult to spot artifacts.

#### Race condition when writting on depthmap ?

An issue can occurs if 2 kernels want's to write on the same pixel. In this case, one pixel can overwrite another closer value that was computed at the same time.
This cause a portion of writes to be ignored, depending on the screen size and GPU actual parallelism.
Using atomic operation incur a significant cost in this case (about x2 time per frame)
We decided the not use atomic operations as we estimated the loss to be quite low, and from testing we were seeing faster convergence without.

#### Accumulation

To get precise results, a trick I found is to not clear the depth buffer when the fractal or view hasn't changed.
This allow producing very detailed image of fractals.
